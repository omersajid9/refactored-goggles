{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.01it/s]\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"google/gemma-2-2b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda\",\n",
    "    token=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13839"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"google-bert/bert-base-uncased\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_streamer = TextStreamer(llm_pipeline.tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"google/gemma-2-2b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda\",\n",
    "    token=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Answer 1]\n",
      "\n",
      "I'm a little late to the party (but I'm not sure if there's a limit for answers here)\n",
      "\n",
      "Embeddings are the way we can use one model to make multiple models, in this case a classification model (<code>kNN</code>) can be used to make a <code>tree</code> model.\n",
      "\n",
      "Embeddings can be thought of as the vectors that hold the information that we want to represent in the tree model.\n",
      "\n",
      "For example, my embeddings would be the words that describe the tree.\n",
      "\n",
      "And if I'm learning about tree rings, the embeddings of the tree would be the width of the rings.\n",
      "\n",
      "The embeddings give the data another point of view that is completely different from the data, so we can use the embeddings to make a new model, that has a completely new point of view.\n",
      "\n",
      "<em>P.S. I made the poem using R to generate the embeddings and then use them to make the tree</em>\n",
      "\n",
      "<code>## Embedding Data\n",
      "dat <- setNames(\n",
      "  c(\n",
      "    rep(\"a\",20),\n",
      "    rep(\"b\",20),\n",
      "    rep(\"c\",20),\n",
      "    rep(\"d\",20),\n",
      "    rep(\"e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write me a poem about Embeddings.\\n\\n[Answer 1]\\n\\nI\\'m a little late to the party (but I\\'m not sure if there\\'s a limit for answers here)\\n\\nEmbeddings are the way we can use one model to make multiple models, in this case a classification model (<code>kNN</code>) can be used to make a <code>tree</code> model.\\n\\nEmbeddings can be thought of as the vectors that hold the information that we want to represent in the tree model.\\n\\nFor example, my embeddings would be the words that describe the tree.\\n\\nAnd if I\\'m learning about tree rings, the embeddings of the tree would be the width of the rings.\\n\\nThe embeddings give the data another point of view that is completely different from the data, so we can use the embeddings to make a new model, that has a completely new point of view.\\n\\n<em>P.S. I made the poem using R to generate the embeddings and then use them to make the tree</em>\\n\\n<code>## Embedding Data\\ndat <- setNames(\\n  c(\\n    rep(\"a\",20),\\n    rep(\"b\",20),\\n    rep(\"c\",20),\\n    rep(\"d\",20),\\n    rep(\"e'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pipeline(\"Write me a poem about Embeddings.\",\n",
    "             max_length=100,\n",
    "             \n",
    "             streamer=text_streamer\n",
    "             )[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:23<00:00, 11.69s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", token=\"\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", revision=\"float16\", token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2026,  102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(\"My\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0])\n",
    "b = np.array([[1.0, 0.0], [1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = [1, 2]\n",
    "b = [[1, 0],\n",
    "     [1, 0]]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Define a train function to be used in different threads\n",
    "def train_fn():\n",
    "    x = torch.ones(5, 5, requires_grad=True)\n",
    "    # forward\n",
    "    y = (x + 3) * (x + 4) * 0.5\n",
    "    # backward\n",
    "    y.sum().backward()\n",
    "    # potential optimizer update\n",
    "    print(\"DONE {i}\")\n",
    "\n",
    "\n",
    "# User write their own threading code to drive the train_fn\n",
    "threads = []\n",
    "for i in range(10):\n",
    "    p = threading.Thread(target=train_fn, args=(i))\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "\n",
    "for p in threads:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is omer\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
